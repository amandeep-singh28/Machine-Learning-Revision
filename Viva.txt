(i) Basic things
->Supervised Learning
It is a type of learning which is performed on labelled data which means that each input has its known correct output.
For example:- Email Spam or Ham, House price prediction, etc.

->Unsupervised Learning
It is a type of learning which is performed on unlabelled data which means that there is no target variable and model tries to learn patterns in it.
For example:- Customer Segmentation.

->Semi-Supervised Learning
Small amount of labelled data and large amount of unlabelled data.
For example:- Image classification where only few images are labelled.

->Reinforced Learning
The model learns through trial and error using rewards and penalties. It receives a reward for correct actions and a penalty for wrong actions.
For example-> Game pkayingh like Chess. 

->How to import inbuilt datasets?
    (i)Using Seaborn library
       import seaborn as sns 
       data = sns.load_dataset('iris')
    (ii)Using scikit-learn library
       from sklearn.datasets import load_iris
       data = load_iris()
       df = pd.DataFrame(data.data, data.feature_names)
       df['target'] = data.target

->How to create a Dataframe?
pd.DataFrame({
    'Col1_name' : [val1, val2, val3],
    'Col2_name' : [val1, val2, val3]
})

->For X(Independent Variable -> Predictors) and y(Dependent Variable -> Target Variable)
X -> 2D array
y -> 1D array

->Label Encoding
It converts categorical values into numerical values. Each unique category gets a unique number.

->Dummy Encoding(One Hot Encoding)
It converts categorical values into binary columns. Each unique value behaves as a new column.

->Discretization
Converting numerical values into categorical values is known as Discretization.

->Feature Scaling
It is used to scaled our values so that one value should not dominate over another value. There are two types of techniques:-
    (i)Normalization(Min-Max Scaling)
        * It is used when our data is not normally distributed.
        * There is a boundation in the output([0,1]).
        * It is used in distance based models.
        * X - Xmin / Xmax - Xmin
    (ii)Standardization(Z-Score)
        * It used when our data is normally distributed.
        * There is no boundation in the output([Min, Max]).
        * It is used in linear models.
        * X - u / sigma

(ii)Evaluation metrics for Regression Models

->R2 squared
It tells how much variance in the target variable is explained by the independent variables.
R2 = 1 - Sum of Square of Residual / Sum of Square of Total -> Sum((yi - yi^) ^ 2) / Sum((yi - yavg) ^ 2)
For example: X = Study Hours, y = Student Marks
             R2_Score = 70%
             It means that 70% of the marks difference are due to study hours. Remaining 30% are due to some other factors.
            
            (i)Problem
               If we keep on adding the columns which have no relationships with other columns than R2 score will increase. Due to this factor Adjusted R2 squared error is their.

->Adjusted R2 squared
It penalise the R2 Score when we add unnecessary columns.
Adjusted R2 = 1 - [(1 - R2) * ((n-1) / (n - p - 1))]
where n = number of observations and p = number of features

->Mean absolute error(MAE)
It measure how far your model's predictions are from the actual values on average.
MAE = E(|yi - yi^|) / n
For example:-Imagine we are predicting the marks of the student. It MAE = 3, it means that our prediction is off by 3 marks from the actual marks
It is less sensitive to outliers.

->Mean squared error(MSE)
It just do the square of the residual error.
MSE = E((yi - yi^)^2) / n
It is more sensitive to outliers.

->Root mean squared error(RMSE)
It is the square root of MSE.
    (i)Why it is better than MAE and MSE?
       Because in MSE the output comes in squared units like marks^2, dollars^2. But in RMSE it penalizes the larger errors + it returns back to the original unit form which is more easy to interpret.

(iii)Evaluation metrics for Classification Models

->Confusion Matrix
It is a table which is used to evaluatethe performance of a classification model.
It shows how well the predicted class matches with the actual class.
TP(True Positive) -> 1(Predicted), 1(Actual)
FP(False Positive) -> 1(Predicted), 0(Actual)
FN(False Negative) -> 0(Predicted), 1(Actual)
TN(True Negative) -> 0(Predicted), 0(Actual)
    (i)Steps
       from sklearn.metrics confusion_matrix, ConfusionMatrixDisplay
       cm = confusion_matrix(y_test, y_pred)
       display = ConfusionMatrixDisplay(confusion_matrix = cm).plot()
       print(display)

->Accuracy Score
How many correctly predicted values are there over total number of values.
Accuracy = TP + TN / TP + TN + FP + FN

->Precision Score
How many predicted positives were actually True Positive(TP).
Precision = TP / TP + FP

->Recall Score
How good we are finding the True Positive values.
Recall = TP / TP + FN

->F1 Score
It tells how strong our model is. It is the harmonic mean of Precision and Recall. It balances both precision and recall and is especially useful when the dataset is imbalanced.
F1 Score = 2 * [(Precision * Recall)/ (Precision + Recall)]

(iv)Types of Models which i have studied so far

->Linear Regression(Supervised Learning)
In this, we predict 1 continous(dependent) value on the basis of 1 independent variables.
y = b1x + b0
    (i)Working
       It works on the concept of OLS(Ordinary Least Square). It means that it finds the regression line by minimizing the sum of squared errors betweem the actual and predicted data points(Residual Error -> E(yi - y^)^2).
       b1 = E(xi - x^) (yi - y^) / E((xi - x^) ^ 2) (Slope Formula)

->Multiple Linear Regression(Supervised Learning)
It is used to predict 1 continos value on the basis of two or more independent variables.
y = b3x3 + b2x2 + b1x1 + b0
    (i)Problem -> How we decide feature selection in Multiple Linear Regression
       Using P-value concept, if p_value < 0.05 feature is significant and if > 0.05 not significant.

->Polynomial Regression(Supervised Learning)
It is also like simple linear regression, but in this the best fit line is formed like a curve which captures more data points as compared to simple linear regression. It is for non-linear data.
    (i)Why we use PolynomialFeatures?
        We use PolynomialFeatures to transform the original features into higher-degree features so that a linear regression model can capture non-linear relationships in the data. Without it, the model behaves like a simple linear regression model.
    (ii)Steps
        from sklearn.linear_model import LinearRegression
        from sklearn.preprocessing import PolynomialFeatures
        ->Independent Varible = 2D
          for example:- X = np.array([[1, 2, 3, 4, 5, 6]])
          This will not work in case of Polynomial Regression. We need to transpose it than it will work correctly
          X = np.array([[1, 2, 3, 4, 5, 6]]).T
        ->Secondly we need to create higher degrees for the X column.
          poly = PolynomialFeatures(degree = n)
          X_poly = poly.fit_transform(X)
        ->Thirdly training
          model = LinearRegression()
          model.fit(X_poly, y)

->Logistic Regression(Supervised Learning)
It is similar to Linear Regression but it works upon Sigmoid function. Unlike Linear regression model which predicts continous value but in case of logistic regression we work upon probabilites. The idea is to compute the probability whether a particular value is going to lie on class 0 class 1.
p = 1 / 1 + e^-y

->K-Nearest Neighbours(KNN) (Supervised Learning)
It is a distance based learning algorithm in which Normalization is required. It is a lazy learner which means that it do not learn anything in the training phase instead it learns at the time of prediction. It works on majority voting.
    (i)Advantages
       - It handles multi class problem efficiently.
       - It works well with non-linear data and complex decision boundaries.
    (ii)Disadvantages
       - More computational expensive because there is no training phase.
       - Lazy Learner.
       - Normalization is required as it is a distance based model.

->Naive Bayes Algorithm (Supervised Classification Algorithm)
It is an algorithm which works on the concept of Bayes theorem that predicts a class by calculating probabilities and assuming that independent features are conditionally independent.
    (i)Bayes theorem
       It describes how to update the probability of an event based on new evidence. It calculates the conditional probability of an event given prior knowledge.
       P(B|A) = P(B) * P(A|B) / P(A)
       Posterior Probability = Prior Probability * Likelihood / Evidence
    (ii)If the dataset is categorical, we will make use of MultinomialNB
       from sklearn.naive_bayes import MultinomialNB
       from sklearn.feature_extraction.text import CountVectorizer {Words -> Number}
       cv = CountVectorizer()
       X_train_cv = cv.fit_transform(X_train)
       X_test_cv = cv.transform(X_test)
    (iii)If the dataset is numerical, we will make use of GaussianNB

->Support Vector Machine (SVM) (Supervised Learning)
It is a supervised learning algorithm which makes use of hyperplane as a decision boundary to separate classes. It separates data by finding the best hyperplane with maximum margin. It works well in high-dimensional spaces.
With the use of kernel trick, SVM can also handle non-linear data.
    (i)Support Vector
       The nearest point through which the marginal plane has passed, that point is known as Support Vector.
    (ii)Hard Marginal Plane
       In which there are no incorrect points between the marginal plane.
    (iii)Soft Marginal Plane
       In which there are incorrect points between the marginal plane.
    (iv)Types Of Kernels:-
       (a)Linear Kernel
          * It does simple dot product.
          * Fastest Kernel.
          * Cannnot capture Non-Linear Data.
       (b)Polynomial Kernel
          * High dimensional space.
          * It handles non-linearity in data.
          * Can become computational expensive at higher degree.
       (c)RBF(Radial Basis Function) -> Gaussian Kernel
          * It can handle more complex and non-linear boundaries.
       (d)Sigmoid Kernel
          * It behaves like a neural network because its formula is similar to tanh activation function which is used in Neural Networks.
          * Adds non-linearity in data.
       (e)Exponential Kernel(Laplacian Kernel)
          * Distance based kernel.
          * It is more robust when our dataset contains outliers.

->Decision Tree (Supervised Learning)
It is a flow chart like structure used for decision making in machine learning.
There are two types of techniques in decision tree:-
    (i)ID3 (Iterative Dichotomiser 3)
       * It is used for classification only.
       * Output -> Decision Tree.
    (ii)CART (Classification & Regression Tree)
       * It is used for classification and regression.
       * Outpur -> Strictly Binary Tree.
    (iii)Working
       * Start with all data at the root node.
       * Pick one feature to split the data so that the resulting child nodes are as pure as possible.
       * For impure node, repeat the process (Select the best feature again and split again).
       * Continue splitting until all nodes are pure.
    (iv)Gini Index and Entropy
       Gini Index and Entropy are measures used in decision trees to check how pure or impure a node is. Both measure class uncertainty, but they differ in their mathematical formulation. Gini Index ranges from 0 to 0.5 (for binary classification), while Entropy ranges from 0 to 1.
    (v)Information Gain
       It selects the best feature by measuring how much entropy is reduced after splitting the data on that feature.
    (vi)Pruning
       It means removing the unnecessary branches from tree. There are two methods:-
       * Post Pruning:- After the tree grows fully, we cut the branches using max_depth parameter.
       * Pre Pruning:- Stop growing tree at the early stage using hyperparameter tuning.
    (vii)Overfitting is the disadvantage in decision tree.

->K-Means Clustering (Unsupervised Learning)
It is an Unsupervised learning algorithm.
    (i)Working
       * Choose the number of clusters.
       * Randomly place centroids.
       * Assign each point to the nearest centroid based on euclidean distance.
       * Recompute centroids, repeat this until we got a perfect centroid which covers more data point.
    (ii)Disadvantage -> Initial assignment of the centroid.
    (iii)Solution -> init = kmeans++. It means that only the first centroid is placed randomly and the second centroid is placed in such a way that it covers more data point.
        Elbow Method -> It helps us to choose the optimal number of clusters.

->Dimensionality Reduction
It means removing unnecessary features from our dataset because irrelevant features can slow down our training speed. It reduces overfitting also.
We can do this with the help of Principal Component Analysis(PCA)
    * It converts original features into new features like PC1, PC2. It tells us that how much information a feature is giving to us like 20%, 60%.
    * Explained Variance Ratio:- It tells us how much information each principal componenet captures.
    (i)Implementation
       from sklearn.decomposition import PCA
       pca = PCA(n_components = 2)
       X_train_pca = pca.fit_transform(X_train)
       X_test_pca = pca.fit_transform(X_test)


