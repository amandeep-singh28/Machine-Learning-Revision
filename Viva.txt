(i) Basic things
->Supervised Learning
It is a type of learning which is performed on labelled data which means that each input has its known correct output.
For example:- Email Spam or Ham, House price prediction, etc.

->Unsupervised Learning
It is a type of learning which is performed on unlabelled data which means that there is no target variable and model tries to learn patterns in it.
For example:- Customer Segmentation.

->Semi-Supervised Learning
Small amount of labelled data and large amount of unlabelled data.
For example:- Image classification where only few images are labelled.

->Reinforced Learning
The model learns through trial and error using rewards and penalties. It receives a reward for correct actions and a penalty for wrong actions.
For example-> Game pkayingh like Chess. 

->How to create a Dataframe?
pd.DataFrame({
    'Col1_name' : [val1, val2, val3],
    'Col2_name' : [val1, val2, val3]
})

->For X(Independent Variable -> Predictors) and y(Dependent Variable -> Target Variable)
X -> 2D array
y -> 1D array

->Label Encoding
It converts categorical values into numerical values. Each unique category gets a unique number.

->Dummy Encoding(One Hot Encoding)
It converts categorical values into binary columns. Each unique value behaves as a new column.

->Discretization
Converting numerical values into categorical values is known as Discretization.

->Feature Scaling
It is used to scaled our values so that one value should not dominate over another value. There are two types of techniques:-
    (i)Normalization(Min-Max Scaling)
        * It is used when our data is not normally distributed.
        * There is a boundation in the output([0,1]).
        * It is used in distance based models.
        * X - Xmin / Xmax - Xmin
    (ii)Standardization(Z-Score)
        * It used when our data is normally distributed.
        * There is no boundation in the output([Min, Max]).
        * It is used in linear models.
        * X - u / sigma

(ii)Evaluation metrics for Regression Models

->R2 squared
It tells how much variance in the target variable is explained by the independent variables.
R2 = 1 - Sum of Square of Residual / Sum of Square of Total -> Sum((yi - yi^) ^ 2) / Sum((yi - yavg) ^ 2)
For example: X = Study Hours, y = Student Marks
             R2_Score = 70%
             It means that 70% of the marks difference are due to study hours. Remaining 30% are due to some other factors.
            
            (i)Problem
               If we keep on adding the columns which have no relationships with other columns than R2 score will increase. Due to this factor Adjusted R2 squared error is their.

->Adjusted R2 squared
It penalise the R2 Score when we add unnecessary columns.
Adjusted R2 = 1 - [(1 - R2) * ((n-1) / (n - p - 1))]
where n = number of observations and p = number of features

->Mean absolute error(MAE)
It measure how far your model's predictions are from the actual values on average.
MAE = E(|yi - yi^|) / n
For example:-Imagine we are predicting the marks of the student. It MAE = 3, it means that our prediction is off by 3 marks from the actual marks
It is less sensitive to outliers.

->Mean squared error(MSE)
It just do the square of the residual error.
MSE = E((yi - yi^)^2) / n
It is more sensitive to outliers.

->Root mean squared error(RMSE)
It is the square root of MSE.
    (i)Why it is better than MAE and MSE?
       Because in MSE the output comes in squared units like marks^2, dollars^2. But in RMSE it penalizes the larger errors + it returns back to the original unit form which is more easy to interpret.

(iii)Evaluation metrics for Classification Models

->Confusion Matrix
It is a table which is used to evaluatethe performance of a classification model.
It shows how well the predicted class matches with the actual class.
TP(True Positive) -> 1(Predicted), 1(Actual)
FP(False Positive) -> 1(Predicted), 0(Actual)
FN(False Negative) -> 0(Predicted), 1(Actual)
TN(True Negative) -> 0(Predicted), 0(Actual)
    (i)Steps
       from sklearn.metrics confusion_matrix, ConfusionMatrixDisplay
       cm = confusion_matrix(y_test, y_pred)
       display = ConfusionMatrixDisplay(confusion_matrix = cm).plot()
       print(display)

->Accuracy Score
How many correctly predicted values are there over total number of values.
Accuracy = TP + TN / TP + TN + FP + FN

->Precision Score
How many predicted positives were actually True Positive(TP).
Precision = TP / TP + FP

->Recall Score
How good we are finding the True Positive values.
Recall = TP / TP + FN

->F1 Score
It tells how strong our model is. It is the harmonic mean of Precision and Recall. It balances both precision and recall and is especially useful when the dataset is imbalanced.
F1 Score = 2 * [(Precision * Recall)/ (Precision + Recall)]

(iv)Types of Models which i have studied so far

->Linear Regression(Supervised Learning)
In this, we predict 1 continous(dependent) value on the basis of 1 independent variables.
y = b1x + b0
    (i)Working
       It works on the concept of OLS(Ordinary Least Square). It means that it finds the regression line by minimizing the sum of squared errors betweem the actual and predicted data points(Residual Error -> E(yi - y^)^2).
       b1 = E(xi - x^) (yi - y^) / E((xi - x^) ^ 2) (Slope Formula)

->Multiple Linear Regression(Supervised Learning)
It is used to predict 1 continos value on the basis of two or more independent variables.
y = b3x3 + b2x2 + b1x1 + b0
    (i)Problem -> How we decide feature selection in Multiple Linear Regression
       Using P-value concept, if p_value < 0.05 feature is significant and if > 0.05 not significant.

->Polynomial Regression(Supervised Learning)
It is also like simple linear regression, but in this the best fit line is formed like a curve which captures more data points as compared to simple linear regression. It is for non-linear data.
    (i)Why we use PolynomialFeatures?
        We use PolynomialFeatures to transform the original features into higher-degree features so that a linear regression model can capture non-linear relationships in the data. Without it, the model behaves like a simple linear regression model.
    (ii)Steps
        from sklearn.linear_model import LinearRegression
        from sklearn.preprocessing import PolynomialFeatures
        ->Independent Varible = 2D
          for example:- X = np.array([[1, 2, 3, 4, 5, 6]])
          This will not work in case of Polynomial Regression. We need to transpose it than it will work correctly
          X = np.array([[1, 2, 3, 4, 5, 6]]).T
        ->Secondly we need to create higher degrees for the X column.
          poly = PolynomialFeatures(degree = n)
          X_poly = poly.fit_transform(X)
        ->Thirdly traning patterns
          model = LinearRegression()
          model.fit(X_poly, y)

->Logistic Regression(Supervised Learning)
It is similar to Linear Regression but it works upon Sigmoid function. Unlike Linear regression model which predicts continous value but in case of logistic regression we work upon probabilites. The idea is to compute the probability whether a particular value is going to lie on class 0 class 1.
p = 1 / 1 + e^-y

